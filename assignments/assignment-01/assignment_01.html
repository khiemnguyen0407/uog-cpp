<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Lecturer in Multiscale Materials">

<title>Assignment 01</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="assignment_01_files/libs/clipboard/clipboard.min.js"></script>
<script src="assignment_01_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="assignment_01_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="assignment_01_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="assignment_01_files/libs/quarto-html/popper.min.js"></script>
<script src="assignment_01_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="assignment_01_files/libs/quarto-html/anchor.min.js"></script>
<link href="assignment_01_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="assignment_01_files/libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="assignment_01_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="assignment_01_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="assignment_01_files/libs/bootstrap/bootstrap-e55939d46d72fd9061d3bd89bd7a8c5f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-full">

<main class="content column-page" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Assignment 01</h1>
</div>



<div class="quarto-title-meta column-page">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Khiem Nguyen <br> Lecturer in Multiscale Materials <br> <a href="mailto:khiem.nguyen@glasgow.ac.uk" class="email">khiem.nguyen@glasgow.ac.uk</a> </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="foreword" class="level1">
<h1>Foreword</h1>
<section id="story-behind-this-assignment" class="level2">
<h2 class="anchored" data-anchor-id="story-behind-this-assignment">Story behind this assignment</h2>
<p>You heard stories from your parents like <a href="https://www.youtube.com/shorts/v0XKgCfFruw">this ‚Äì click on me!!!</a> I was much luckier than that; I could learn Pascal in highschool, and then C++ at <span class="math inline">\(18\)</span>. Nevertheless, <span class="math inline">\(\ldots\)</span></p>
<ul>
<li>I did not have GenAI chatbots like ChatGPT, Gemini, CoPilot. In fact, <em>stackoverflow</em> (created by Jeff Atwood and Joel Spolsky in 2008) was not even a thing at that time</li>
<li>The <span class="math inline">\(100\%\)</span> course grade was entirely decided by one single paper exam. If we were lucky, roughly <span class="math inline">\(30\%\)</span> would be assigned to a team project.</li>
</ul>
<p><em>Yes, you did not read it wrong. It was not long time ago.</em> We completed the course by taking one single paper exam in which we must literally write C++ code on paper. Henceforth, the only one to prepare for the exam is to practice as much as possible to get ready for one single shot. There are both advantages and disadvantages:</p>
<ul>
<li><strong>Advantage</strong>: You have to learn basic details with little instructions. You cannot get away with good grades/passing grades with the so-called sufficient knowledge.</li>
<li><strong>Disadvantage</strong>: Learning curve is steep and it took sometimes too much time to finish a simple task.</li>
</ul>
<p>If you still don‚Äôt belive me, read the file <code>programming_exams.html</code>.</p>
</section>
<section id="pov-of-a-lecturer" class="level2">
<h2 class="anchored" data-anchor-id="pov-of-a-lecturer">POV of a lecturer</h2>
<p>First of all, I have taught several courses involving heavy programming course work since 2017. I notice, actually know for sure, from my own teaching and marking experience within the last two years that I have marked ChatGPT‚Äôs submissions more than students‚Äô submissions.</p>
<blockquote class="blockquote">
<p><strong>That‚Äôs fine, it‚Äôs your future, not mine</strong> üòè.</p>
</blockquote>
<p>At one point, I know that I cannot stop you to use ChatGPT ‚Äì So I joined the force since 2025 (just roughly half a year). No matter how many exercises I assigned, one could easily solve all of them in less than <span class="math inline">\(1\)</span> hour, without understanding a single line of code. Indeed, I copied/pasted my entire question in ChatGPT or Gemini and got the solution in return without any single error. Here are my own stories:</p>
<ul>
<li>As for ENG3091 last year, several students got less than <span class="math inline">\(5/35\)</span> points for the coding question and less than <span class="math inline">\(3/15\)</span> for questions about C++ programming. Several students got points because they wrote something (I gave <span class="math inline">\(1\)</span> point for making attempt), not because they answered the questions correctly.</li>
<li>If I was not lenient, several students actually got completely <span class="math inline">\(0/50\)</span> for the C++ coding part. Some students wrote literall English in coding questions. Ironically, they finished the mid-term assignments with all A-band grades.</li>
<li>In the Teaching Committee, a few professors stated that for the first time of their teaching experience, they marked <span class="math inline">\(0/100\)</span>.</li>
<li>Indeed, last semester a few students just confessed upfront that they used ChatGPT to solve the assignments and asked for the passing grades instead of trying to answer my questions.</li>
</ul>
</section>
<section id="rationale-on-the-length-of-the-assignment" class="level2">
<h2 class="anchored" data-anchor-id="rationale-on-the-length-of-the-assignment">Rationale on the length of the assignment</h2>
<p><span style="color:red"><em>Why is the assignment so long?</em></span></p>
<p>Here is my experience. If I give one single assignment with a significantly meaningful topic, you will skip all the practice opportunities and finally end up using ChatGPT to solve the final exam ‚Äì Again, I saw and marked more than AI-geneted code than you could imagine.</p>
<p>The first part of the assignment gives you the opportunities to practice writing code in C++. The second part of the assignment introduce you to the simplest knowledge in the machine learning world: Linear Regression.</p>
</section>
<section id="my-advices" class="level2">
<h2 class="anchored" data-anchor-id="my-advices"><span style="color:blue">My advices</span></h2>
<ul>
<li>I encourage you to try to solve the exercises using just <strong>Google</strong>, <strong>stackoverflow</strong>, before trying to use ChatGPT, Gemini or CoPilot.</li>
<li>If you decide to use GenAI Chatbots, you should try to refrain yourself from copy/paste the solution in a bruteforce manner. Instead, you should ask them for knowledge you want to learn.</li>
<li>Keep in mind that you will have to write C++ code on paper in the final paper exam.</li>
<li><span style="color:blue"><em>Good habits compound, so do bad habits.</em></span></li>
</ul>
</section>
</section>
<section id="task-1" class="level1">
<h1>Task 1</h1>
<p>In the first task, you need to write C++ programs to solve <strong>two out of three</strong> problems you have solved in Python.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 66%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Problem</th>
<th>Points</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Fixed-point and Newton-Raphson methods for solving nonlinear equation</td>
<td>15/100</td>
</tr>
<tr class="even">
<td>Rouge-like room and moving the player in the room</td>
<td>15/100</td>
</tr>
<tr class="odd">
<td>The tic-tac-toe game</td>
<td>15/100</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p>Python is not the focus of our course. The problems and their solution in Python code are provided.</p>
</blockquote>
<ul>
<li>Essentially, you learn to translate Python code into C++ code with the knowledge you will learn in this course.</li>
<li>You can use object-oriented programming (OOP) if you want. However, it won‚Äôt give extra points by using OOP.</li>
<li>You won‚Äôt have extra points for solving all three problems.</li>
<li>You need to write <strong>significantly meaningful comments</strong> in your code.</li>
<li>The deadline for this assignment is pretty far. So don‚Äôt worry too much.</li>
<li>Submit the source code. Explain how to compile the source code to obtain the executable files.</li>
</ul>
<blockquote class="blockquote">
<p>Feel free to use GenAI chatbots to learn anything you need.</p>
</blockquote>
</section>
<section id="task-2" class="level1">
<h1>Task 2</h1>
<p><strong>Intro</strong>: This year I decide to roll out a slightly more challenging problems but also more fun to do as compared to those given in the prior years. My aim is to provide you an opportunity to learn machine learning. I hope that in the end of the Assignment 2, we can build a program to classify hand-written digits: <span class="math inline">\(0, 1, \ldots, 9\)</span>.</p>
<p>In this task, we will build a linear regression model, which is the first step of building a complex neural network.</p>
<section id="problem-description" class="level2">
<h2 class="anchored" data-anchor-id="problem-description">Problem description</h2>
<p>You can read this <a href="https://en.wikipedia.org/wiki/Linear_regression">wikipedia: Linear Regression</a> for multi-dimensional linear regression. But my description will use different notations.</p>
<p>Imagine that an Estate Agent/Glasgow Council gives us a task of predicting the house price in Glasgow by using various features of a house/flat such as number of bedrooms, total living area, distance to schools and city center and so on. Mathematically speaking, we need to build a prediction model that can be represented by a mapping <span class="math display">\[
y = f(x_1, x_2, \ldots, x_n), \quad y=\text{house price}
\]</span> and <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span> correspond to the features of the house. For example, <span class="math inline">\(x_1\)</span> is the total living area, <span class="math inline">\(x_2\)</span> is the number of bedrooms, <span class="math inline">\(x_3\)</span> is the distance to the city center and so on. The prediction model, represented by <span class="math inline">\(f\)</span>, estimates the relationship between a scalar response (dependent variable <span class="math inline">\(y\)</span>) and one or more explanatory variables (independent variable <span class="math inline">\(\mathbf{x}\)</span>) The ultimate goal is to seek the function <span class="math inline">\(f\)</span>.</p>
<p><strong>Example</strong></p>
<p>We go around the city of Glasgow and collect the dataset <span class="math inline">\(\mathcal{D}\)</span> of <span class="math inline">\(m\)</span> data points:</p>
<p><span id="eq-dataset-multiple-features"><span class="math display">\[
\mathcal{D} = \big\{ (\mathbf{x}^{(1)}, y^{(1)}), (\mathbf{x}^{(2)}, y^{(2)}), \ldots (\mathbf{x}^{(m)}, y^{(m)})\big\}.
\tag{1}\]</span></span> Note that one single data point comprises both <span class="math inline">\(\mathbf{x} \in \mathbb{R}^{n}\)</span> and <span class="math inline">\(y \in \mathbb{R}\)</span> components. If we try to predict the house price by using only one feature such as total living area <span class="math inline">\(x_1\)</span>, then we have a dataset as <span class="math display">\[
\mathcal{D} = \big\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots (x^{(m)}, y^{(m)})\big\}.
\]</span> If we try to predict the house price by using two features <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> (number of bedrooms), the we have a dataset as</p>
<p><span class="math display">\[
\mathcal{D} = \big\{ ((x_1^{(1)}, x_2^{(1)}), y^{(1)}), ((x_1^{(2)}, x_2^{(2)}), y^{(2)}), \ldots ((x_1^{(m)}, x_2^{(m)}), y^{(m)})\big\}.
\]</span> Now, you can imagine the kind of dataset we are dealing with if we decide to use more than <span class="math inline">\(2\)</span> features to predict the house price. In fact, <span class="math inline">\(x_1, x_2, \ldots\)</span> are called <strong>features</strong></p>
</section>
<section id="simple-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="simple-linear-regression">Simple linear regression</h2>
<p>Let us simplify the above problem to prediction of the house price based on the total living area of the house. In this case, the vector <span class="math inline">\(\mathbf{x}\)</span> becomes a scalar <span class="math inline">\(x\)</span>, and our dataset looks like <span class="math display">\[
\mathcal{D} = \big\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots (x^{(m)}, y^{(m)})\big\}.
\]</span></p>
<p>The simplest prediction model we can build is the <strong>simple linear regression</strong>. In statistics, simple linear regression is a linear regression model with a single independent variable.</p>
<p>Consider the model function <span id="eq-simple-linear-regression-model-function"><span class="math display">\[
(M):\qquad \widehat{y} = f_{w, b}(x) := w x + b
\tag{2}\]</span></span> which describes a line with the slope <span class="math inline">\(w\)</span> and <span class="math inline">\(y\)</span>-intercept value <span class="math inline">\(b\)</span>. We wan to look for <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> so that the line <strong>fits</strong> the dataset <span class="math inline">\(\mathcal{D}\)</span> as much as possible. Since the model will be trained or learned through an iterative process, the parameters <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> are called <strong>learnable parameters</strong>.</p>
<p>If we can manage to do so, given a particular total living area <span class="math inline">\(A\)</span>, we can make the prediction that the house price should be approximate to <span class="math inline">\(f_{w, b}(A)\)</span>. That‚Äôs it; we collect the money from the Agency and Glasgow Council ‚Äì Smiley.</p>
<p>See the code snippet and the output to understand my point:</p>
<div id="4c2b30b9" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Generate Synthetic Data</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)  <span class="co"># For reproducibility</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> np.random.rand(<span class="dv">100</span>, <span class="dv">1</span>)  <span class="co"># 100 random points (Features)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Formula: y = 4 + 3x + Gaussian noise</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">4</span> <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> X <span class="op">+</span> <span class="fl">0.25</span><span class="op">*</span>np.random.randn(<span class="dv">100</span>, <span class="dv">1</span>) </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Initialize and Train the Model</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression()</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Make Predictions</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Create new data points to plot the regression line</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> np.array([[<span class="dv">0</span>], [<span class="dv">2</span>]])</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>y_predict <span class="op">=</span> model.predict(X_new)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Results</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Intercept (b): </span><span class="sc">{</span>model<span class="sc">.</span>intercept_[<span class="dv">0</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Coefficient (m): </span><span class="sc">{</span>model<span class="sc">.</span>coef_[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Visualize the result</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, y, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Actual Data'</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>plt.plot(X_new, y_predict, color<span class="op">=</span><span class="st">'red'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'Regression Line'</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Independent Variable (X)"</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Dependent Variable (y)"</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Linear Regression with scikit-learn"</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Intercept (b): 4.0538
Coefficient (m): 2.9425</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="assignment_01_files/figure-html/cell-2-output-2.png" width="585" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="building-a-linear-regression-model" class="level2">
<h2 class="anchored" data-anchor-id="building-a-linear-regression-model">Building a linear regression model</h2>
<p>Let us assume that we have only <span class="math inline">\(2\)</span> data points. Obviously, the best line to fit two data points is the line going through both data points. However, when we have more than <span class="math inline">\(2\)</span> data points and they do not lie on one single line, we have to find a way to measure the <strong>fitness</strong> of the model.</p>
<p>Using the model <span class="math inline">\((M)\)</span>, the absolute error between the prediction <span class="math inline">\(\widehat{y}^{(1)} = f_{w,b}(x^{(1)})\)</span> and the observed value <span class="math inline">\(y^{(1)}\)</span> is</p>
<p><span class="math display">\[
|\widehat{y}^{(i)} - y^{(i)}| = |f_{w,b}(x^{(i)}) - y^{(i)}|
\]</span></p>
<p>The average error of all the absolute errors between the prediction of the model at the data input <span class="math inline">\(x^{(i)}\)</span> and the data output <span class="math inline">\(y^{(i)}\)</span> is given by</p>
<p><span class="math display">\[
Q(m, w) = \frac{1}{m}\sum\limits_{i=1}^{m}\big|\widehat{y}^{(i)}  - y^{(i)} \big| = \frac{1}{m}\sum\limits_{i=1}^{m}\big|w x^{(i)} + b  - y^{(i)} \big|.
\]</span> We see that this average error <span class="math inline">\(Q\)</span> is just a function of the parameters <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span>, because <span class="math inline">\(x^{(i)}\)</span> and <span class="math inline">\(y^{(i)}\)</span> are given from the dataset. To find the best fit of the model, we only need to minimize the the average error <span class="math inline">\(Q(w, b)\)</span>:</p>
<p><span class="math display">\[
\min\limits_{w, b} Q(w, b)\quad\longrightarrow w, b
\]</span></p>
<p>However, it turns out that minimizing the function <span class="math inline">\(Q(w, b)\)</span> is not convenient because it involves absolutes of multiple values. To overcome this issue, we can consider the squared error for one single data point <span class="math display">\[
\varepsilon^2 = (\widehat{y}^{(i)} - y^{(i)})^2 = (w x^{(i)} + b - y^{(i)})^2
\]</span> and consider the mean squared error function <span class="math inline">\(\mathcal{L}\)</span> instead of the function <span class="math inline">\(Q\)</span> as follows <span id="eq-loss-function"><span class="math display">\[
\mathcal{L}(w, b) = \frac{1}{2m} \sum\limits_{i=1}^{m} (\widehat{y}^{(i)} - y^{(i)})^2 =\frac{1}{2m} \sum\limits_{i=1}^{m} (w x^{(i)} + b - y^{(i)})^2
\tag{3}\]</span></span> Function <span class="math inline">\(\mathcal{L}\)</span> is called <strong>loss function</strong> in the optimization theory and machine learning community. Note that the factor <span class="math inline">\(2\)</span> is added for convenience of computing the gradient of <span class="math inline">\(\mathcal{L}\)</span> and it does not change the optimal learning parameters. By minimizing the loss function <span class="math inline">\(\mathcal{L}\)</span> with respect to two parameters <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span>, we obtain the linear regression model that best fit the dataset <span class="math inline">\(\mathcal{D}\)</span>:</p>
<p><span class="math display">\[
\min\limits_{w, b}\mathcal{L}(w, b)\quad\longrightarrow w, b
\]</span></p>
</section>
<section id="computation-of-the-model-parameters" class="level2">
<h2 class="anchored" data-anchor-id="computation-of-the-model-parameters">Computation of the model parameters</h2>
<p>No matter how many data points <span class="math inline">\(\mathcal{D}\)</span> has, the loss function <span class="math inline">\(\mathcal{L}(w, b)\)</span> is obviously a quadratic function of <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span>. If you don‚Äôt believe me, try to compute <span class="math inline">\(\mathcal{L}(w, b)\)</span> with <span class="math inline">\(2\)</span> data points, and with <span class="math inline">\(3\)</span> data poitns and then with <span class="math inline">\(4\)</span> data points to convince yourself. For this reason, the minimization of <span class="math inline">\(\mathcal{L}\)</span> can be easily computed and there is a unique optimal solution <span class="math inline">\((w^{\ast}, b^{\ast})\)</span> to minimize <span class="math inline">\(\mathcal{L}(w, b)\)</span>.</p>
<section id="a-normal-equation" class="level3">
<h3 class="anchored" data-anchor-id="a-normal-equation">A ‚Äì Normal equation</h3>
<p>Indeed, from basic mathematics , you should have learned that the optimal solution <span class="math inline">\((w^{\ast}, b^{\ast})\)</span> is obtained by solving the the system of equations <span class="math display">\[
\nabla \mathcal{L}(w, b) = 0 \quad\Leftrightarrow\quad \left\{\begin{aligned}
\frac{\partial \mathcal{L}}{\partial w} &amp;= 0 \\[6pt]
\frac{\partial \mathcal{L}}{\partial b} &amp;= 0
\end{aligned}\right.
\]</span> By carrying out some algebraic computation, we end up with the solution<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>: <span id="eq-normal-equation-1"><span class="math display">\[
\begin{aligned}
w^{\ast} &amp;= \frac{\sum\limits_{i=1}^{m}(x^{(i)} - \overline{x})(y^{(i)} - \overline{y})}{\sum\limits_{i=1}^{m} (x^{(i)} - \overline{x})^2}, \\[3pt]
b^{\ast} &amp;= \overline{y} - w^{\ast} \cdot \overline{x},
\end{aligned}
\tag{4}\]</span></span> where <span class="math inline">\(\overline{x}\)</span> and <span class="math inline">\(\overline{y}\)</span> are the average of the <span class="math inline">\(x^{(i)}\)</span> and <span class="math inline">\(y^{(i)}\)</span>, respectively: <span class="math display">\[
\overline{x} = \frac{1}{m}\sum\limits_{i=1} x^{(i)},\qquad \overline{y} = \frac{1}{m}\sum\limits_{i=1} y^{(i)}
\]</span></p>
<p>The above result can be expanded to the following expression: <span id="eq-normal-equation-2"><span class="math display">\[
\begin{aligned}
w^{\ast} &amp;= \frac{m \sum_{i=1}^{m} x^{(i)} y^{(i)} - \sum_{i=1}^{m} x^{(i)} \sum_{i=1}^{m} y^{(i)}}{m \sum_{i=1}^{m} (x^{(i)})^2 - \left( \sum_{i=1}^{m} x^{(i)} \right)^2} \\[3pt]
b^{\ast} &amp;= \frac{\sum_{i=1}^{m} y^{(i)} \sum_{i=1}^{m} (x^{(i)})^2 - \sum_{i=1}^{m} x^{(i)} \sum_{i=1}^{m} x^{(i)} y^{(i)}}{m \sum_{i=1}^{m} (x^{(i)})^2 - \left( \sum_{i=1}^{m} x^{(i)} \right)^2}
\end{aligned}
\tag{5}\]</span></span></p>
<p>The solution formulas given by <a href="#eq-normal-equation-1" class="quarto-xref">Equation&nbsp;4</a> and <a href="#eq-normal-equation-2" class="quarto-xref">Equation&nbsp;5</a> are equivalent. Either of them is called <strong>Normal Equation</strong>.</p>
</section>
<section id="b-gradient-descent-method" class="level3">
<h3 class="anchored" data-anchor-id="b-gradient-descent-method">B ‚Äì Gradient descent method</h3>
<p>To prepare for the future assignment 02, we shall learn gradient descent method to minimize the loss function given by <a href="#eq-loss-function" class="quarto-xref">Equation&nbsp;3</a> ‚Äì <a href="https://en.wikipedia.org/wiki/Gradient_descent">Wikipedia: Gradient Descent</a></p>
<p>You will learn later that the classification problem using the logistic regression method cannot be solved easily by deriving an analytical set of equations. There is no single ‚ÄúNormal Equation‚Äù for logistic regression.</p>
<p>In the world of machine learning, <strong>Gradient Descent</strong> is the primary algorithm used to ‚Äútrain‚Äù models by minimizing error. This is an iterative approach to find the best possible parameters.</p>
<section id="the-intuition-hiking-in-the-fog" class="level4">
<h4 class="anchored" data-anchor-id="the-intuition-hiking-in-the-fog">The intuition: Hiking in the Fog</h4>
<p>Imagine you are standing on a mountain in a thick fog. You want to reach the very bottom of the valley, but you cannot see where it is. To get there, you might:</p>
<ol type="1">
<li>Feel the slope of the ground under your feet.</li>
<li>Take a small step in the direction where the ground slopes <em>downward</em> most steeply.</li>
<li>Repeat this process until the ground feels flat under your feet.</li>
</ol>
<p>In this analogy:</p>
<ul>
<li>The Mountain is the Cost Function (the measure of how much error your model is making).</li>
<li>The Bottom of the Valley is the point where the error is at its minimum.</li>
<li>The <strong>Steepness</strong> is the <strong>Gradient</strong> (the derivative of the cost function).</li>
</ul>
</section>
<section id="gradient-descent-formula" class="level4">
<h4 class="anchored" data-anchor-id="gradient-descent-formula">Gradient descent formula</h4>
<p>Assume that we have a function of multiple parameters (variables) <span class="math inline">\(J = J(\theta_{1}, \ldots, \theta_{n})\)</span>. Note that we use the term ‚Äúparameters‚Äù to reflect our linear regression model. What we explain here is valid for a function of multiple variables. Our aim is to minimize <span class="math inline">\(J\)</span> with respect to the parameters <span class="math inline">\(\Theta\)</span> using an interative process. To update a set of parameters <span class="math inline">\(\boldsymbol{\Theta} = (\theta_1, \ldots, \theta_n)\)</span>, we use the following update rule:</p>
<p><span id="eq-gradient-descent"><span class="math display">\[
\boldsymbol{\Theta}^{\text{new}} = \boldsymbol{\Theta}^{\text{old}} - \eta \cdot \nabla J(\boldsymbol{\Theta}^{\text{old}}).
\tag{6}\]</span></span> This update will be repeated until</p>
<ul>
<li>the value of <span class="math inline">\(J(\boldsymbol{\Theta})\)</span> does not change much, or</li>
<li>all the components of the gradient <span class="math inline">\(\nabla J(\boldsymbol{\Theta})\)</span> are almost equal to <span class="math inline">\(0\)</span>.</li>
</ul>
<p>Here is what each part represents:</p>
<ul>
<li><span class="math inline">\(\boldsymbol{\Theta}\)</span> ‚Äì The parameters we are trying to optimize.</li>
<li><span class="math inline">\(\eta\)</span> ‚Äì The <strong>learning rate</strong>.<br>
This value determines how big of a step we take in each iteration.</li>
<li><span class="math inline">\(\nabla J(\boldsymbol{\Theta})\)</span> ‚Äì The gradient.<br>
This is the derivative of the cost function <span class="math inline">\(J\)</span> with respect to <span class="math inline">\(\boldsymbol{\Theta}\)</span>. It tells us the slope at our current position.</li>
<li>The <strong>minus sign</strong> (<span class="math inline">\(-\)</span>) is crucial.<br>
It ensures we move <em>opposite</em> the gradient (downhill) rather than with it (uphill).</li>
</ul>
<p>As for our case of linear regression model, the vector of parameters is <span class="math inline">\(\boldsymbol{\Theta} = (w, b)\)</span> and the function to be optimized is the loss function <span class="math inline">\(\mathcal{L}(w, b)\)</span>. The gradient descent rule <a href="#eq-gradient-descent" class="quarto-xref">Equation&nbsp;6</a> for our case becomes <span class="math display">\[
\begin{aligned}
w^\text{new} &amp;= w^\text{old} - \eta\,\frac{\partial \mathcal{L}}{\partial w}(w^\text{old}, b^\text{old}) \\[3pt]
b^\text{new} &amp;= b^\text{old} - \eta\,\frac{\partial \mathcal{L}}{\partial b}(w^\text{old}, b^\text{old}) \\[3pt]
\end{aligned}
\]</span></p>
<p>We can prove that the derivatives of <span class="math inline">\(\mathcal{L}\)</span> following <a href="#eq-loss-function" class="quarto-xref">Equation&nbsp;3</a> with respect to <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> are given by<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>: <span id="eq-gradient-loss-function-simple-lr"><span class="math display">\[
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial w} &amp;= \frac{1}{m} \sum\limits_{i=1}^{m}(\widehat{y}^{(i)} - y^{(i)}) \, x^{(i)} = \frac{1}{m}\sum\limits_{i=1}^{m}(w x^{(i)} + b - y^{(i)})\, x^{(i)}, \\[3pt]
\frac{\partial \mathcal{L}}{\partial b} &amp;= \frac{1}{m} \sum\limits_{i=1}^{m}(\widehat{y}^{(i)} - y^{(i)})\cdot 1 = \frac{1}{m}\sum\limits_{i=1}^{m}(w x^{(i)} + b - y^{(i)}), \\[3pt]
\end{aligned}
\tag{7}\]</span></span></p>
</section>
</section>
</section>
<section id="multiple-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="multiple-linear-regression">Multiple linear regression</h2>
<p>Leveraging on the knowledge we have just learned for single-variable linear regression, we now revisit the prediction model that employs multiple features. In this case, we use the dataset given by <a href="#eq-dataset-multiple-features" class="quarto-xref">Equation&nbsp;1</a>. Our goal is to build a <strong>multiple linear regression</strong> model.</p>
<p>Recalling the model function <a href="#eq-simple-linear-regression-model-function" class="quarto-xref">Equation&nbsp;2</a>, we can derive the single-variable linear regression to multiple linear regression by extending <a href="#eq-simple-linear-regression-model-function" class="quarto-xref">Equation&nbsp;2</a> to the following model function <span class="math display">\[
(M):\qquad \widehat{y} = f_{\mathbf{w}, b}(\mathbf{x}) := w_1 x_1 + \ldots + w_n x_n + b = \mathbf{w} \cdot \mathbf{x} + b,
\]</span> where <span class="math inline">\(\mathbf{w}\cdot \mathbf{x} = w_1 x_1 + \ldots + w_n x_n\)</span> denote the dot product between two vectors <span class="math inline">\(\mathbf{w} = (w_1, \ldots, w_n)\)</span> and <span class="math inline">\(\mathbf{x} = (x_1, \ldots, x_n)\)</span>.</p>
<p>The loss function <span class="math inline">\(\mathcal{L}\)</span> to be minimized is constructed in the same fashion as <a href="#eq-loss-function" class="quarto-xref">Equation&nbsp;3</a>. The only difference is that it is now a function of all the learning parameters <span class="math inline">\(\mathbf{w}= (w_1, \ldots, w_n)\)</span> and <span class="math inline">\(b\)</span>. <span class="math display">\[
\mathcal{L}(\mathbf{w}, b) = \frac{1}{2m} \sum\limits_{i=1}^{m} (\widehat{y}^{(i)} - y^{(i)})^2 = \frac{1}{2m} \sum\limits_{i=1}^{m} (\mathbf{w}\cdot \mathbf{x}^{(i)} + b - y^{(i)})^2.
\]</span> We aim to minimize the loss function <span class="math inline">\(\mathcal{L}(\mathbf{w}, b)\)</span> to obtain the optimal parameters <span class="math inline">\(\mathbf{w}\)</span>, <span class="math inline">\(b\)</span>: <span class="math display">\[
\min\limits_{\mathbf{w}, b} \mathcal{L}(\mathbf{w}, b) \longrightarrow \mathbf{w}^{\ast}, b^{\ast}
\]</span> Again, the parameters <span class="math inline">\(\mathbf{w}\)</span> and <span class="math inline">\(b\)</span> are called <strong>learnable parameters</strong>.</p>
<p>Before discussing the gradient descent method for this minimization problem, we note that the loss function is still a quadratic function of the parameters <span class="math inline">\(w_1, \ldots, w_n\)</span> and <span class="math inline">\(b\)</span>. For this reason, there is one unique optimal solution <span class="math inline">\(\mathbf{w}^{\ast}, b^{\ast}\)</span> for this minimization problem. This optimal solution can be derived by a normal equation; and it is not difficult to derive it. Nevertheless, we are not interested in pursuing this. Instead, we will solve this minimization problem using the gradient descent method.</p>
<section id="gradient-descent-method" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent-method">Gradient descent method</h3>
<p>The gradient descent method is still carried out as before following the updating rule of <a href="#eq-gradient-descent" class="quarto-xref">Equation&nbsp;6</a>. We now only need to compute the gradient <span class="math inline">\(\nabla \mathcal{L}\)</span> with respect to <span class="math inline">\(\mathbf{w}\)</span> and <span class="math inline">\(b\)</span>. It turns out that the computation of <span class="math inline">\(\nabla \mathcal{L}\)</span> is just a natural extension of <a href="#eq-gradient-loss-function-simple-lr" class="quarto-xref">Equation&nbsp;7</a><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>:</p>
<!-- $$
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial w_{1}} &= \frac{1}{m} \sum\limits_{i=1}^{m}( \widehat{y}{(i)} - y^{(i)})\, x_{1}^{(i)} = \frac{1}{m} \sum\limits_{i=1}^{m}(\mathbf{w}\cdot \mathbf{x}^{(i)} + b - y^{(i)}) x_{1}^{(i)}, \\[3pt]
\frac{\partial \mathcal{L}}{\partial w_{2}} &= \frac{1}{m} \sum\limits_{i=1}^{m}( \widehat{y}{(i)} - y^{(i)})\, x_{2}^{(i)} = \frac{1}{m} \sum\limits_{i=1}^{m}(\mathbf{w}\cdot \mathbf{x}^{(i)} + b - y^{(i)}) x_{2}^{(i)}, \\[3pt]
\vdots \\[3pt]
\frac{\partial \mathcal{L}}{\partial w_{n}} &= \frac{1}{m} \sum\limits_{i=1}^{m}( \widehat{y}{(i)} - y^{(i)})\, x_{n}^{(i)} = \frac{1}{m} \sum\limits_{i=1}^{m}(\mathbf{w}\cdot \mathbf{x}^{(i)} + b - y^{(i)}) x_{n}^{(i)}, \\[3pt]
\frac{\partial \mathcal{L}}{\partial b} &= \frac{1}{m} \sum\limits_{i=1}^{m}(\widehat{y}^{(i)} - y^{(i)})\cdot 1 = \frac{1}{m}\sum\limits_{i=1}^{m}(\mathbf{w} \cdot \mathbf{x}^{(i)} + b - y^{(i)}).
\end{aligned}
$$ -->
<p><span class="math display">\[
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial w_{1}} &amp;= \frac{1}{m} \sum\limits_{i=1}^{m}( \widehat{y}{(i)} - y^{(i)})\, x_{1}^{(i)} = \frac{1}{m} \sum\limits_{i=1}^{m}(\mathbf{w}\cdot \mathbf{x}^{(i)} + b - y^{(i)}) x_{1}^{(i)}, \\[3pt]
\frac{\partial \mathcal{L}}{\partial w_{2}} &amp;= \frac{1}{m} \sum\limits_{i=1}^{m}( \widehat{y}{(i)} - y^{(i)})\, x_{2}^{(i)} = \frac{1}{m} \sum\limits_{i=1}^{m}(\mathbf{w}\cdot \mathbf{x}^{(i)} + b - y^{(i)}) x_{2}^{(i)}, \\[3pt]
\vdots \\[3pt]
\frac{\partial \mathcal{L}}{\partial w_{n}} &amp;= \frac{1}{m} \sum\limits_{i=1}^{m}( \widehat{y}{(i)} - y^{(i)})\, x_{n}^{(i)} = \frac{1}{m} \sum\limits_{i=1}^{m}(\mathbf{w}\cdot \mathbf{x}^{(i)} + b - y^{(i)}) x_{n}^{(i)}, \\[3pt]
\frac{\partial \mathcal{L}}{\partial b} &amp;= \frac{1}{m} \sum\limits_{i=1}^{m}(\widehat{y}^{(i)} - y^{(i)})\cdot 1 = \frac{1}{m}\sum\limits_{i=1}^{m}(\mathbf{w} \cdot \mathbf{x}^{(i)} + b - y^{(i)}).
\end{aligned}
\]</span></p>
</section>
</section>
<section id="tasks" class="level2">
<h2 class="anchored" data-anchor-id="tasks">Tasks</h2>
<p><span style="color:red"><strong>Learning process</strong></span>&nbsp; First, use Gemini Guided Learning Mode to learn the following techniques/concepts to help you solve the problem:</p>
<ol type="1">
<li>How to create a vector using <code>std::vector</code>.</li>
<li>How to create a vector of vectors of the same size, which is matrix. This step is for solving multiple linear regression.</li>
<li>How to create random numbers in monder C++ (C++ 11 and later) using <code>std::random_device</code>, <code>std::mt19937</code>, <code>std::uniform_int_distribution</code>, and maybe more.</li>
<li>How to write data into a file using <code>fstream</code>.</li>
<li>Maybe more <span class="math inline">\(\ldots\)</span></li>
</ol>
<blockquote class="blockquote">
<p>ChatGPT/Gemini can easily give you the solution to the entire problem. But you should learn how to use them üòâ. If I give you a problem that ChatGPT/Gemini cannot solve it, then we are all F_up.</p>
</blockquote>
<p>Next, you need to write a C++ program to build a linear regression model by implementing both methods: Particularly, you should do the following</p>
<ol type="1">
<li>Generate a dataset <span class="math inline">\(\mathcal{D}\)</span> following a simple linear function <span class="math inline">\(y = k\cdot x + m\)</span>.</li>
<li>Add noise to both the <span class="math inline">\(x^{(i)}\)</span> and <span class="math inline">\(y^{(i)}\)</span> values in the generated dataset.<br>
<strong>Note</strong>: Be careful with the amount of noise you add. You don‚Äôt want it looks wild.</li>
<li>Derive the linear regression model for your generated dataset by computing the model parameters <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span>. You must implement both methods:
<ul>
<li>Analytical method using <strong>normal equation</strong></li>
<li><strong>Gradient descent</strong> method</li>
</ul></li>
<li>Compare the results obtained by the two above-mentioned methods.</li>
<li>Repeat the steps <span class="math inline">\((1), (2)\)</span> and <span class="math inline">\((3)\)</span> but for multiple linear regression. For running your model, you only need to focus on <span class="math inline">\(2\)</span>-variable linear regression. That is, we have <span class="math inline">\(\mathbf{x}^{(i)} = (x_1^{(i)}, x_2^{(i)}) \in \mathbb{R}^{2}\)</span> and <span class="math inline">\(y^{(i)} \in \mathbb{R}\)</span>.</li>
<li>Compare the learning parameters <span class="math inline">\(\mathbf{w}^{\ast} = (w_1^{\ast}, w_2^{\ast})\)</span> and <span class="math inline">\(b^{\ast}\)</span> with the hypothetical weights you used to generate the data. That is, if you generate the dataset based on the plane <span class="math inline">\(y = w_1 x_1 + w_2 x_2 + b\)</span>, then you compare <span class="math inline">\(\mathbf{w}^{\ast}\)</span> versus <span class="math inline">\(\mathbf{w}\)</span> and <span class="math inline">\(b^{\ast}\)</span> versus <span class="math inline">\(b\)</span>.</li>
<li><strong>Optional</strong>: Visualize your result by using Python to understand the whole idea of this problem. Unfortunately, plotting in C++ is a technically involved task.</li>
</ol>
<p>Python is not the focus of our course. The following code may give you an idea about how the <code>.csv</code> file look like.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Read the CSV generated by C++</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'data.csv'</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot to verify</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>plt.scatter(df[<span class="st">'x'</span>], df[<span class="st">'y'</span>], alpha<span class="op">=</span><span class="fl">0.6</span>, label<span class="op">=</span><span class="st">'Data with Noise'</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Dataset Generated from C++'</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'y'</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
</section>
<section id="marking-scheme" class="level1">
<h1>Marking scheme</h1>
<p><span style="color:red"><strong>Total points</strong></span>: <span class="math inline">\(100/100\)</span></p>
<ol type="1">
<li><strong>Task</strong> <span class="math inline">\(1\)</span>: Total <span class="math inline">\(30/100\)</span>, therefore <span class="math inline">\(15/100\)</span> for each subtask.</li>
<li><strong>Task</strong> <span class="math inline">\(2\)</span>: Total <span class="math inline">\(40/100\)</span>
<ul>
<li><span class="math inline">\(10/100\)</span> for generating data with noise (for single-variable linear regression, and for two-variable linear regression)</li>
<li><span class="math inline">\(10/100\)</span> for implementating the normal equation</li>
<li><span class="math inline">\(10/100\)</span> for implementing the gradient descent method for simple linear regression problem (single variable linear regression).</li>
<li><span class="math inline">\(10/100\)</span> for extending the gradent descent method to multiple linear regression problem.<br>
Of course, the result for multiple linear regression problem should already cover the result for simple linear regression. <em>In all cases, you must run your program with data and output the learnable parameters.</em></li>
</ul></li>
<li><strong>Report</strong>: <span class="math inline">\(30/100\)</span><br>
<strong>Requirements</strong>:
<ul>
<li>You need to write <em>significantly meaningful comments</em> in your source code.</li>
<li>You explain what you have learned.</li>
<li>Why you write the code the way you did.</li>
</ul></li>
</ol>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>You can try to prove this by yourself but it is not the core of this assignment.<a href="#fnref1" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn2"><p>You can try to derive this formulation. It is ridiculously easy.<a href="#fnref2" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn3"><p>You can try to derive this formulation. It is ridiculously easy.<a href="#fnref3" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>